{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-f4YAw7EVmP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and extract text from a PDF file\n",
        "def read_pdf(file_path):\n",
        "    # Open the file in read-binary ('rb') mode because PDFs are binary files.\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)  # Create a PDF reader object to read the PDF.\n",
        "        content = [page.extract_text() for page in reader.pages]  # Extract text from each page. This is crucial as we need textual data for analysis.\n",
        "        return \" \".join(content)  # Join all the text from all pages to create a continuous string. This makes processing easier."
      ],
      "metadata": {
        "id": "2j22kmffFP0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess and read multiple PDF documents specified in 'specific_files'\n",
        "def preprocess_specific_docs(folder_path, specific_files):\n",
        "    docs = {}  # Initialize an empty dictionary to store the contents of each document. It's a convenient way to keep data organized by filenames.\n",
        "    for filename in specific_files:\n",
        "        file_path = os.path.join(folder_path, filename + '.pdf')  # Construct the full file path for each document. This step is necessary to locate each file in the system.\n",
        "        docs[filename] = read_pdf(file_path)  # Store the content of each PDF in the dictionary, with the filename as the key. This is for easy retrieval later.\n",
        "    return docs  # Return the dictionary containing the contents of the documents."
      ],
      "metadata": {
        "id": "Dc7gQ7E2FTAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the cosine similarity between the test document vector and each document vector\n",
        "def calculate_cosine_similarity(vectorizer, docs, test_doc_vector):\n",
        "    similarities = {}  # Initialize an empty dictionary to store the similarity scores. This helps in comparing each document with the test document.\n",
        "    for doc_name, doc_vector in docs.items():\n",
        "        cosine_sim = cosine_similarity(test_doc_vector, doc_vector)  # Calculate the cosine similarity. This step is vital as cosine similarity measures how similar the documents are in terms of content.\n",
        "        similarities[doc_name] = cosine_sim[0][0]  # Store the similarity score in the dictionary. We are only interested in the actual similarity value.\n",
        "    return similarities  # Return the dictionary of similarity scores."
      ],
      "metadata": {
        "id": "Rpzx6TIcFt7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of specific document names for training. These documents are pre-selected and categorized, serving as a basis for comparison with the test document.\n",
        "training_docs = [\n",
        "    'Maths1', 'Maths2', 'Maths3', 'Maths4', 'Maths5',\n",
        "    'Bio1', 'Bio2', 'Bio3', 'Bio4', 'Bio5',\n",
        "    'Finance1', 'Finance2', 'Finance3', 'Finance4', 'Finance5'\n",
        "]\n"
      ],
      "metadata": {
        "id": "7YP2ZUfoF7N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the folder path to the current directory. This is where the script will look for the PDF files.\n",
        "folder_path = '/content'\n",
        "\n",
        "# Preprocess and read the specified training documents. This step is crucial for converting the raw PDFs into a format (text) that can be analyzed.\n",
        "docs = preprocess_specific_docs(folder_path, training_docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJGz33UoF_zi",
        "outputId": "2703746f-adff-49af-cbf8-44c1775c8242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(9, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(15, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(21, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(27, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(33, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(39, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(45, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(51, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(57, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(63, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(69, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(75, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(81, 0, 133609524344960)]\n",
            "WARNING:PyPDF2._cmap:unknown widths : \n",
            "[0, IndirectObject(87, 0, 133609524344960)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEQokQ_FLl9r",
        "outputId": "b21ddc4c-b0ba-48b2-abf5-3e7d09548045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TF-IDF vectorizer object. TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document, which is part of a corpus. This is crucial for converting text data into numerical data that can be processed.\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Transform the training documents into TF-IDF vectors. This conversion is essential for preparing the data for similarity calculation.\n",
        "doc_vectors = vectorizer.fit_transform(docs.values())"
      ],
      "metadata": {
        "id": "lEVEyBbhLmtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNG-zXV7NZZO",
        "outputId": "e945ee00-4b0b-469d-db11-08bbfac42976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 16882)\t0.0031738991447470746\n",
            "  (0, 7082)\t0.0024594906347017867\n",
            "  (0, 8492)\t0.0020415884461292195\n",
            "  (0, 6385)\t0.0028337205330362636\n",
            "  (0, 2898)\t0.006347798289494149\n",
            "  (0, 9467)\t0.006347798289494149\n",
            "  (0, 2536)\t0.0015150939540463\n",
            "  (0, 6495)\t0.0018827094077568838\n",
            "  (0, 16625)\t0.0031738991447470746\n",
            "  (0, 1926)\t0.0027559969561745078\n",
            "  (0, 0)\t0.0016236862575566525\n",
            "  (0, 17312)\t0.0031738991447470746\n",
            "  (0, 16109)\t0.0031738991447470746\n",
            "  (0, 14128)\t0.0014168602665181318\n",
            "  (0, 2399)\t0.0018827094077568838\n",
            "  (0, 9047)\t0.0015150939540463\n",
            "  (0, 13708)\t0.002229502464091588\n",
            "  (0, 12966)\t0.0031738991447470746\n",
            "  (0, 5064)\t0.0031738991447470746\n",
            "  (0, 2040)\t0.0027559969561745078\n",
            "  (0, 4413)\t0.0027559969561745078\n",
            "  (0, 1783)\t0.0031738991447470746\n",
            "  (0, 1698)\t0.0027559969561745078\n",
            "  (0, 14874)\t0.0027559969561745078\n",
            "  (0, 1200)\t0.0031738991447470746\n",
            "  :\t:\n",
            "  (14, 2882)\t0.007438223072851873\n",
            "  (14, 15014)\t0.0032601231555539067\n",
            "  (14, 9164)\t0.0019254812534818988\n",
            "  (14, 16048)\t0.0052938929323676056\n",
            "  (14, 7377)\t0.040435106323119877\n",
            "  (14, 3273)\t0.03465866256267418\n",
            "  (14, 16854)\t0.03273318130919228\n",
            "  (14, 2902)\t0.011552887520891394\n",
            "  (14, 9288)\t0.09434858142061305\n",
            "  (14, 2825)\t0.01540385002785519\n",
            "  (14, 15745)\t0.15211301902507002\n",
            "  (14, 3324)\t0.002182593446731748\n",
            "  (14, 14996)\t0.03720459132555678\n",
            "  (14, 3257)\t0.007034470697607512\n",
            "  (14, 15624)\t0.05968991885793887\n",
            "  (14, 8821)\t0.14248561275766053\n",
            "  (14, 11415)\t0.0019254812534818988\n",
            "  (14, 5647)\t0.009917630763802497\n",
            "  (14, 16737)\t0.0038509625069637976\n",
            "  (14, 11732)\t0.10012502518105874\n",
            "  (14, 4141)\t0.02118029378830089\n",
            "  (14, 15512)\t0.4120529882451264\n",
            "  (14, 2383)\t0.002182593446731748\n",
            "  (14, 11618)\t0.2888221880222848\n",
            "  (14, 3268)\t0.011321857722652078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and vectorize the test document separately. This separation ensures that the test document is not influencing the TF-IDF calculation of the training set.\n",
        "test_doc_name = 'test_document.pdf'\n",
        "test_doc_content = read_pdf(os.path.join(folder_path, test_doc_name))\n",
        "test_doc_vector = vectorizer.transform([test_doc_content])"
      ],
      "metadata": {
        "id": "8x3negC4MN5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the cosine similarity between the test document and training documents. This step is the core of the script, determining how similar the test document is to each training document.\n",
        "similarities = calculate_cosine_similarity(vectorizer, dict(zip(docs.keys(), doc_vectors)), test_doc_vector)"
      ],
      "metadata": {
        "id": "ZBxdH-UDM-79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the document with the highest similarity score. This step identifies which category the test document is most likely to belong to.\n",
        "most_similar_doc = max(similarities, key=similarities.get)\n",
        "# Extract the category from the filename of the most similar document. The naming convention of the training documents is used here to determine the category.\n",
        "category = most_similar_doc.split('1')[0]  # Assuming the category is indicated by the first part of the filename\n",
        "print(f\"The test document is most similar to category: {category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEiIDaAPNp8S",
        "outputId": "e9a17d01-d8fb-4291-ecb9-87b204346c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test document is most similar to category: Maths4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the cosine similarities between the test document and each document in the training set\n",
        "average_similarities = calculate_cosine_similarity(vectorizer, dict(zip(docs.keys(), doc_vectors)), test_doc_vector)"
      ],
      "metadata": {
        "id": "V4rws7mjNvcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This step calculates the average similarity of the test document with each category.\n",
        "# It's important because it assesses how similar the test document is to each category as a whole, rather than to individual documents.\n",
        "# This approach can sometimes provide a more accurate categorization, especially when the test document shares similarities with multiple documents in a single category.\n"
      ],
      "metadata": {
        "id": "JRGnRGlBN2_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the category with the highest average similarity score\n",
        "most_similar_category = max(average_similarities, key=average_similarities.get)\n",
        "print(f\"Average similarities with each category: {average_similarities}\")\n",
        "print(f\"The test document is most similar to the category: {most_similar_category} (highest average similarity)\")"
      ],
      "metadata": {
        "id": "mcZbDQMCN9In",
        "outputId": "b0958463-87d7-40fd-e5aa-4635c99a1db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average similarities with each category: {'Maths1': 0.56531073689647, 'Maths2': 0.34087477732528065, 'Maths3': 0.4766718724517198, 'Maths4': 0.7333055496855635, 'Maths5': 0.5642719282123181, 'Bio1': 0.600026119031007, 'Bio2': 0.6081045738301034, 'Bio3': 0.635890712946899, 'Bio4': 0.7051929630629038, 'Bio5': 0.627522008609555, 'Finance1': 0.685658033878281, 'Finance2': 0.5210012093508289, 'Finance3': 0.6629233092468131, 'Finance4': 0.6463871133154039, 'Finance5': 0.5619454895337315}\n",
            "The test document is most similar to the category: Maths4 (highest average similarity)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SI4ZFUuRN9sX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}